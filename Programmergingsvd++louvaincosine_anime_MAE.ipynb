{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9293045-7964-42ae-bf42-a4fcb3607f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genre data        Josei  Hentai  Ecchi  Romance  Samurai  Sci-Fi  Magic  Drama  Mecha  \\\n",
      "B446       0       0      0        0        0       0      0      0      0   \n",
      "B6464      0       0      0        0        0       0      0      0      0   \n",
      "B3245      0       0      0        0        0       0      0      0      0   \n",
      "B5119      0       0      0        0        0       0      0      0      0   \n",
      "B6924      0       1      0        0        0       0      0      0      0   \n",
      "...      ...     ...    ...      ...      ...     ...    ...    ...    ...   \n",
      "B1731      0       0      0        0        0       0      0      0      0   \n",
      "B6315      0       0      0        0        0       0      0      0      0   \n",
      "B5572      0       0      0        0        0       0      0      0      0   \n",
      "B51        0       0      0        0        0       0      0      0      0   \n",
      "B6281      0       0      0        0        0       0      0      0      0   \n",
      "\n",
      "       Parody  ...  Vampire  Thriller  Shounen Ai  Action  Slice of Life  \\\n",
      "B446        0  ...        0         0           0       1              0   \n",
      "B6464       0  ...        0         0           0       0              0   \n",
      "B3245       0  ...        0         0           0       0              0   \n",
      "B5119       0  ...        0         0           0       1              0   \n",
      "B6924       0  ...        0         0           0       0              0   \n",
      "...       ...  ...      ...       ...         ...     ...            ...   \n",
      "B1731       0  ...        0         0           0       1              0   \n",
      "B6315       0  ...        0         0           0       1              0   \n",
      "B5572       0  ...        0         0           0       0              0   \n",
      "B51         0  ...        0         0           0       1              0   \n",
      "B6281       0  ...        0         0           0       0              0   \n",
      "\n",
      "       Harem  Shoujo Ai  Shounen  Psychological  Horror  \n",
      "B446       0          0        0              0       0  \n",
      "B6464      0          0        1              0       0  \n",
      "B3245      0          0        0              0       0  \n",
      "B5119      0          0        0              0       0  \n",
      "B6924      0          0        0              0       0  \n",
      "...      ...        ...      ...            ...     ...  \n",
      "B1731      0          0        0              0       0  \n",
      "B6315      0          0        0              0       0  \n",
      "B5572      0          0        0              0       0  \n",
      "B51        0          0        0              0       0  \n",
      "B6281      0          0        0              0       0  \n",
      "\n",
      "[7157 rows x 43 columns]\n",
      "(7157, 43)\n",
      "{'steps': [], 2: [array([0.25778238, 0.65108803, 1.04443593, 1.43784098, 1.83135534,\n",
      "       2.22503231, 2.61898359, 3.01352414, 3.40922017, 3.8068993 ,\n",
      "       4.20743571])], 3: [array([0.34584437, 0.73055593, 1.11530348, 1.50088603, 1.88761372,\n",
      "       2.27532649, 2.66463611, 3.05506409, 3.44790551, 3.84433107,\n",
      "       4.24417601])], 4: [array([0.33181742, 0.70651035, 1.08125129, 1.45607839, 1.83188181,\n",
      "       2.2084602 , 2.58545283, 2.9635436 , 3.34421871, 3.72723299,\n",
      "       4.11342352])], 5: [array([0.45333535, 0.8117197 , 1.17017483, 1.52995361, 1.89165111,\n",
      "       2.25465668, 2.62109455, 2.98950812, 3.36133746, 3.73792979,\n",
      "       4.11850033])], 6: [array([0.66689579, 0.98640023, 1.30620476, 1.62649896, 1.94853428,\n",
      "       2.2723101 , 2.59810618, 2.92710488, 3.26036309, 3.60023207,\n",
      "       3.947133  ])], 7: [array([0.48211991, 0.84695579, 1.21191642, 1.57705946, 1.94254745,\n",
      "       2.30850278, 2.67535065, 3.04409683, 3.41625164, 3.79300079,\n",
      "       4.17496919])], 8: [array([0.68743076, 0.98412336, 1.28112207, 1.57893005, 1.87771352,\n",
      "       2.1772259 , 2.47879749, 2.78294677, 3.09238959, 3.40893247,\n",
      "       3.73376287])], 9: [array([0.66109445, 0.99553107, 1.3300698 , 1.66475303, 1.99985592,\n",
      "       2.33546024, 2.67187839, 3.01009784, 3.35268298, 3.70212115,\n",
      "       4.06051814])], 10: [array([0.47029211, 0.80353426, 1.13694223, 1.47059051, 1.80459562,\n",
      "       2.13925069, 2.47474495, 2.81230517, 3.15293546, 3.49789318,\n",
      "       3.84800896])], 11: [array([0.48337153, 0.83931391, 1.19541405, 1.55169555, 1.90831332,\n",
      "       2.26551328, 2.62353783, 2.98357348, 3.34708972, 3.71544436,\n",
      "       4.08958007])], 12: [array([0.49566353, 0.81309315, 1.13068538, 1.44844469, 1.76674607,\n",
      "       2.08549889, 2.40528629, 2.72740754, 3.0528956 , 3.38371681,\n",
      "       3.71985246])], 13: [array([0.56975984, 0.91098156, 1.25254492, 1.59454863, 1.93703432,\n",
      "       2.28044948, 2.62536373, 2.97331085, 3.32609841, 3.684701  ,\n",
      "       4.04889714])], 14: [array([0.60879215, 0.92039153, 1.23226116, 1.54441049, 1.85708711,\n",
      "       2.17067385, 2.48589497, 2.80374073, 3.12667618, 3.455859  ,\n",
      "       3.79152818])], 15: [array([0.6095405 , 0.95611849, 1.30284858, 1.6499019 , 1.99748299,\n",
      "       2.34600311, 2.69624359, 3.049655  , 3.40868581, 3.77387001,\n",
      "       4.14473565])], 16: [array([0.49409499, 0.8284755 , 1.16291551, 1.49755303, 1.83309937,\n",
      "       2.16912073, 2.5060104 , 2.84471501, 3.18621092, 3.53223212,\n",
      "       3.88328208])], 17: [array([0.52269619, 0.85624023, 1.18993536, 1.52388166, 1.85823388,\n",
      "       2.19327447, 2.52966839, 2.86854448, 3.2119833 , 3.56072014,\n",
      "       3.91416796])], 18: [array([0.40684824, 0.76349214, 1.12025525, 1.47721046, 1.83483776,\n",
      "       2.19312273, 2.55269863, 2.91419428, 3.27852219, 3.64665135,\n",
      "       4.01893959])], 19: [array([0.73890305, 1.06428693, 1.38992444, 1.7158934 , 2.04279133,\n",
      "       2.37115897, 2.70103275, 3.03400495, 3.37142388, 3.71519624,\n",
      "       4.06667857])], 20: [array([0.60274067, 0.90044468, 1.19823763, 1.49643535, 1.79817422,\n",
      "       2.10339023, 2.41127871, 2.72254213, 3.03985159, 3.36335002,\n",
      "       3.69298605])], 21: [array([0.64640417, 0.95627125, 1.26641741, 1.577152  , 1.88854477,\n",
      "       2.20117125, 2.51628627, 2.83387132, 3.15611948, 3.48522411,\n",
      "       3.82220602])], 22: [array([0.49481562, 0.84925817, 1.203842  , 1.55864076, 1.91381318,\n",
      "       2.26959401, 2.6263656 , 2.98495182, 3.34676907, 3.71318622,\n",
      "       4.08463708])], 23: [array([0.5705273 , 0.90537815, 1.24041602, 1.57579533, 1.91223044,\n",
      "       2.25007686, 2.5895797 , 2.93196717, 3.27894557, 3.63153651,\n",
      "       3.99002628])], 24: [array([0.57825784, 0.90779929, 1.23753441, 1.5676017 , 1.89842264,\n",
      "       2.23051883, 2.56476164, 2.90210556, 3.24425631, 3.59270923,\n",
      "       3.94728117])], 25: [array([0.65877164, 0.96517345, 1.27182578, 1.57909621, 1.88732123,\n",
      "       2.1965005 , 2.50713728, 2.82060479, 3.1384362 , 3.46285088,\n",
      "       3.79381626])]}\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "#from networkx.algorithms import bipartite\n",
    "from networkx.algorithms import bipartite\n",
    "from networkx.algorithms.community.quality import modularity\n",
    "from surprise import Dataset, NormalPredictor, Reader\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise import accuracy, SVDpp\n",
    "from surprise.model_selection import KFold\n",
    "#from networkx.algorithms.community import greedy_modularity_communities\n",
    "import community\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xlsxwriter\n",
    "import math\n",
    "\n",
    "G = nx.Graph()\n",
    "# Read the file and add edges between users and items\n",
    "with open('animeupdate.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        u1, v1, r= line.strip().split('\\t')\n",
    "        u=str(\"A\")+u1\n",
    "        v=str(\"B\")+v1\n",
    "        G.add_node(u, bipartite=0)\n",
    "        G.add_node(v, bipartite=1)\n",
    "        G.add_edge(u, v)\n",
    "\n",
    "# Extract two node sets of the bipartite graph\n",
    "users = set(n for n, d in G.nodes(data=True) if d[\"bipartite\"] == 0)\n",
    "items = set(G) - users\n",
    "\n",
    "def louvain_algorithm(G, k):\n",
    "    # First, run the initial Louvain algorithm to obtain the first partition\n",
    "    partition = community.community_louvain.best_partition(G)\n",
    "    \n",
    "    # Initialize the list of partitions\n",
    "    partitions = [list() for _ in range(max(partition.values())+1)]\n",
    "    \n",
    "    # Assign each node to its corresponding community\n",
    "    for node, community_id in partition.items():\n",
    "        partitions[community_id].append(node)\n",
    "    \n",
    "    # Iterate until the desired number of components is reached\n",
    "    while len(partitions) < k:\n",
    "        # Compute the modularity score for the current partition\n",
    "        #prev_modularity = community.modularity(partition, G)\n",
    "        prev_modularity = community.modularity(partition, G)\n",
    "        # Iterate over each community in the current partition\n",
    "        for community_id in range(len(partitions)):\n",
    "            # Create a subgraph of the current community\n",
    "            subgraph = G.subgraph(partitions[community_id])\n",
    "            \n",
    "            # Compute the modularity score for each possible new community for this subgraph\n",
    "            best_modularity = -1\n",
    "            best_community_id = community_id\n",
    "            for neighbor in subgraph.nodes:\n",
    "                neighbor_community_id = partition[neighbor]\n",
    "                if neighbor_community_id != community_id:\n",
    "                    new_partition = partition.copy()\n",
    "                    new_partition[neighbor] = community_id\n",
    "                    #new_modularity = community.modularity(new_partition, G)\n",
    "                    new_modularity = community.modularity(new_partition, G)\n",
    "                    if new_modularity > best_modularity:\n",
    "                        best_modularity = new_modularity\n",
    "                        best_community_id = neighbor_community_id\n",
    "            \n",
    "            # If a better partition was found, update the partition\n",
    "            if best_modularity > prev_modularity:\n",
    "                for node in partitions[community_id]:\n",
    "                    partition[node] = best_community_id\n",
    "                partitions[best_community_id].extend(partitions[community_id])\n",
    "                partitions[community_id].clear()\n",
    "        \n",
    "        # Compute the modularity score for the current partition\n",
    "        #new_modularity = community.modularity(partition, G)\n",
    "        new_modularity = community.modularity(partition, G)\n",
    "        # If no further improvement can be made, break out of the loop\n",
    "        if new_modularity == prev_modularity:\n",
    "            break\n",
    "    \n",
    "    # Remove any empty partitions and return the final list of partitions\n",
    "    partitions = [p for p in partitions if p]\n",
    "    \n",
    "    # If the number of partitions is greater than k, merge partitions until only k remain\n",
    "    while len(partitions) > k:\n",
    "        # Compute the modularity score for each possible pair of partitions to merge\n",
    "        best_modularity = -1\n",
    "        best_partition_ids = None\n",
    "        for i in range(len(partitions)):\n",
    "            for j in range(i+1, len(partitions)):\n",
    "                new_partition = partition.copy()\n",
    "                for node in partitions[i]:\n",
    "                    new_partition[node] = j\n",
    "                #new_modularity = community.modularity(new_partition, G)\n",
    "                new_modularity = community.modularity(new_partition, G)\n",
    "                if new_modularity > best_modularity:\n",
    "                    best_modularity = new_modularity\n",
    "                    best_partition_ids = (i, j)\n",
    "        \n",
    "        # Merge the best pair of partitions\n",
    "        i, j = best_partition_ids\n",
    "        partition = {node: j if partition[node] == i else partition[node] for node in partition}\n",
    "        partitions[j].extend(partitions[i])\n",
    "        partitions[i].clear()\n",
    "        partitions = [p for p in partitions if p]\n",
    "    \n",
    "    # Return the final list of partitions\n",
    "    return partitions[:k]\n",
    "\n",
    "\n",
    "#print(G)\n",
    "df = pd.read_csv('animeupdate.txt', delimiter='\\t', header=None, names=['user', 'item', 'rating'])\n",
    "\n",
    "#data.rename(columns=lambda x: 'B'+ x, inplace = True)\n",
    "df['user'] = 'A' + df['user'].astype(str)\n",
    "df['item'] = 'B' + df['item'].astype(str)\n",
    "#print(df)\n",
    "matrix = df.pivot(index='user', columns='item', values='rating')\n",
    "#print(matrix)\n",
    "# # Fill missing values with zeros\n",
    "matrix = matrix.fillna(0)\n",
    "\n",
    "# Print the resulting matrix\n",
    "#print(type(matrix))\n",
    "# Convert DataFrame to numpy array\n",
    "matrix_R = matrix.to_numpy()\n",
    "\n",
    "\n",
    "# Read the file content\n",
    "file_path = 'anime_info.txt'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    animes_data = file.readlines()\n",
    "    \n",
    "\n",
    "# Define the list of genres\n",
    "genres_list = [\n",
    "    \"Josei\", \"Hentai\", \"Ecchi\", \"Romance\", \"Samurai\", \"Sci-Fi\", \"Magic\", \"Drama\", \"Mecha\", \"Parody\", \"Demons\", \n",
    "\"Game\", \"Seinen\", \"Martial Arts\", \"Yuri\", \"Dementia\", \"Shoujo\", \"Military\", \"Fantasy\", \"Adventure\", \"Historical\", \n",
    "\"Yaoi\", \"Music\", \"Sports\", \"Super Power\", \"Kids\", \"Space\", \"Police\", \"Mystery\", \"Cars\", \"Comedy\", \"Supernatural\", \n",
    "\"School\", \"Vampire\", \"Thriller\", \"Shounen Ai\", \"Action\", \"Slice of Life\", \"Harem\", \"Shoujo Ai\", \"Shounen\", \n",
    "\"Psychological\", \"Horror\"\n",
    "]\n",
    "\n",
    "# Convert the 'items' set to a list\n",
    "items_list=list(items)\n",
    "df1 = pd.DataFrame(0, index=items_list, columns=genres_list) \n",
    "#print(df1)\n",
    "\n",
    "\n",
    "for line in animes_data:\n",
    "    parts = line.strip().split(\"\\t\")\n",
    "    animes_id = str(parts[0])  # Convert to 0-based index\n",
    "    genres = parts[2].split(',')\n",
    "    #print(genres)\n",
    "    \n",
    "    for genre in genres:\n",
    "            if 'B'+animes_id in items and genre in genres_list:\n",
    "                df1.at['B'+animes_id, str(genre)] = 1\n",
    "\n",
    "\n",
    "#print(\"genre\",df1)\n",
    "genre_data=df1\n",
    "print('genre data', genre_data)\n",
    "print(genre_data.shape)   \n",
    "\n",
    "\n",
    "max_communities=25\n",
    "iterations=25\n",
    "D={}\n",
    "D[\"steps\"]=[]\n",
    "#cramse={}\n",
    "#c=1\n",
    "for com in range(2, max_communities+1):\n",
    "    D[com]=[]\n",
    "    avg=0\n",
    "    rows, cols = (iterations,10)\n",
    "    arr = [[0]*cols]*rows\n",
    "    for it in range(0, iterations):\n",
    "        P = louvain_algorithm(G, com)\n",
    "        #print(P)\n",
    "        dfmat={}\n",
    "        for part in P:\n",
    "        #print(\"part matrix1\",part)\n",
    "            part_users = [u for u in part if u in matrix.index]\n",
    "            part_items = [i for i in part if i in matrix.columns]\n",
    "            #print(\"Users\",part_users)\n",
    "            #print(\"items\",part_items)\n",
    "            part_matrix = matrix.loc[part_users, part_items]\n",
    "            part_genre=genre_data.loc[part_items,genres_list]\n",
    "            r,c=part_matrix.shape\n",
    "            R = part_matrix.to_numpy()\n",
    "            genre=part_genre.to_numpy()\n",
    "            #print(\"R\",R.shape)\n",
    "            #print(\"genre\", genre.shape)\n",
    "            # Construct the user profile matrix\n",
    "            user_profile_matrix = np.zeros((len(part_users), 43))\n",
    "            for user_index in range(len(part_users)):\n",
    "                for genre_index in range(43):\n",
    "                    user_profile_matrix[user_index, genre_index] = np.dot(R[user_index, :], genre[:, genre_index])\n",
    "            #print(\"user_profile_matrix\", user_profile_matrix.shape)\n",
    "            dotproduct_matrix = np.dot(user_profile_matrix, genre.T)\n",
    "            #print(\"dotproduct_matrix\", dotproduct_matrix.shape)\n",
    "            user_magnitudes = np.sqrt(np.sum(user_profile_matrix**2, axis=1))\n",
    "            #print(\"usermagniudes\", user_magnitudes.shape)\n",
    "            item_magnitudes = np.sqrt(np.sum(genre**2, axis=1))\n",
    "            #print(\"itemmagniudes\", item_magnitudes.shape)\n",
    "            numerator_matrix = dotproduct_matrix\n",
    "            cosine_similarity_matrix = np.zeros_like(dotproduct_matrix, dtype=float)\n",
    "            for user_index in range(len(part_users)):\n",
    "                for item_index in range(len(part_items)):\n",
    "                    denominator = user_magnitudes[user_index] * item_magnitudes[item_index]\n",
    "                    #print(denominator)\n",
    "                    if denominator!=0:\n",
    "                        cosine_similarity_matrix[user_index, item_index] = numerator_matrix[user_index, item_index] / denominator\n",
    "                    else:\n",
    "                        cosine_similarity_matrix[user_index, item_index] = 0\n",
    "                    # Display the cosine similarity matrix\n",
    "            #print(\"\\nCosine Similarity Matrix:\")\n",
    "            #print(cosine_similarity_matrix)\n",
    "            #print(cosine_similarity_matrix.shape)\n",
    "            # Scale the values from range 0-1 to range 1-5\n",
    "            min_val = 0\n",
    "            #print(min_val)\n",
    "            max_val = 1\n",
    "            #print(max_val)\n",
    "            ratingmin=1\n",
    "            ratingmax=10\n",
    "            updated_cosine_similarity_matrix = ratingmin + (ratingmax-ratingmin) * ((cosine_similarity_matrix - min_val) / (max_val - min_val))\n",
    "            #print(\"\\nupdated_Cosine Similarity Matrix:\")\n",
    "            #print(updated_cosine_similarity_matrix)\n",
    "            #print(updated_cosine_similarity_matrix.shape)        \n",
    "            user_items = pd.DataFrame(R, columns=part_items, index=part_users).reset_index()\n",
    "            user_items = pd.melt(user_items, id_vars=[\"index\"], value_name=\"rating\", var_name=\"item\")\n",
    "            user_items.columns = [\"user\", \"item\", \"rating\"]\n",
    "            #user_items = user_items[user_items[\"rating\"] != 0].reset_index(drop=True)\n",
    "            #user_items.columns = [\"user\", \"item\", \"rating\"]\n",
    "            df = user_items[user_items.rating != 0].reset_index(drop=True)\n",
    "            reader = Reader(rating_scale=(1, 5))  #change the rating range even data set changes\n",
    "            data = Dataset.load_from_df(df[[\"user\", \"item\", \"rating\"]], reader)\n",
    "            predictions_df = pd.DataFrame(index=df['user'].unique(), columns=df['item'].unique())\n",
    "            name=25\n",
    "            kf = KFold(n_splits=2)\n",
    "            n_factors=10\n",
    "            if r>1 and c>1:\n",
    "                algo = SVDpp(n_factors)\n",
    "                for trainset, testset in kf.split(data):\n",
    "                #print(type(trainset))\n",
    "                # train and test algorithm.\n",
    "                    algo.fit(trainset)\n",
    "                    predictions = algo.test(testset)\n",
    "                    for pred in predictions:\n",
    "                        predictions_df.loc[pred.uid, pred.iid] = pred.est\n",
    "                predictions_df.fillna(0, inplace=True)\n",
    "                dfmat[name]=predictions_df\n",
    "                nR = predictions_df.to_numpy()\n",
    "            #print(\"Rating matrix for \"+str(name))\n",
    "            #print(nR.shape)\n",
    "            else:\n",
    "                nR = part_matrix.to_numpy()\n",
    "                dfmat[name]=predictions_df\n",
    "            #print(\"Rating matrix for \"+str(name))\n",
    "            #print(nR)        \n",
    "            #print(nR.shape)\n",
    "            #cramse[c]=[]\n",
    "        L=[]\n",
    "        for w1 in [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]:\n",
    "            sum2=0\n",
    "            pred_matrix_final=w1*(updated_cosine_similarity_matrix)+(1-w1)*(nR)\n",
    "            for i in range(len(pred_matrix_final)):\n",
    "                for j in range(len(pred_matrix_final[0])):\n",
    "                    sum2 = sum2+abs(pred_matrix_final[i][j] - R[i][j])\n",
    "            MAE_final =(sum2/(len(R[0])*len(R)))      \n",
    "            L.append(MAE_final)\n",
    "        #c=c+1\n",
    "        #D[w1]=[]\n",
    "        arr[it]=L\n",
    "    D[com].append(np.average(arr, axis=0))\n",
    "    #print(cramse)\n",
    "print(D)\n",
    "#df = pd.DataFrame.from_dict({(i,j): D[i][j]\n",
    "#                             for i in D.keys()\n",
    "#                             for j in range(len(D[i]))},\n",
    "#                            orient='index')\n",
    "# Create a Pandas Excel writer using xlsxwriter as the engine\n",
    "writer = pd.ExcelWriter(\"25iterationscombinedLouSVD++cosine_anime_k=10_MAE.xlsx\", engine='xlsxwriter')\n",
    "\n",
    "# Loop over the dictionary keys\n",
    "for key in D.keys():\n",
    "    # Convert the dictionary subset to a DataFrame\n",
    "    df = pd.DataFrame(D[key])\n",
    "\n",
    "    # Write DataFrame to a sheet named 'Sheet_i'\n",
    "    df.to_excel(writer, sheet_name=f'Sheet_{key}', index=False)\n",
    "\n",
    "# Save the Excel file\n",
    "writer.close()\n",
    "#df =pd.DataFrame(D)\n",
    "#print(df)\n",
    "#df.to_csv('combinedLouMFcosine_ml-100K_k=10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011dc154-d765-418e-a142-3c0307cdc5f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
