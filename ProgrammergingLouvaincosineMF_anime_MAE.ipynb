{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b889697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genre data        Josei  Hentai  Ecchi  Romance  Samurai  Sci-Fi  Magic  Drama  Mecha  \\\n",
      "B1814      0       0      0        0        0       0      0      0      0   \n",
      "B6470      0       0      0        0        0       0      0      0      0   \n",
      "B893       0       0      0        0        0       0      0      0      0   \n",
      "B1991      0       0      0        0        0       0      0      0      0   \n",
      "B1150      0       0      0        0        0       0      0      0      0   \n",
      "...      ...     ...    ...      ...      ...     ...    ...    ...    ...   \n",
      "B5021      0       0      0        0        0       0      0      0      0   \n",
      "B6078      0       0      0        0        0       0      0      0      0   \n",
      "B469       0       0      0        0        0       0      0      0      0   \n",
      "B6254      0       0      0        0        0       0      0      0      0   \n",
      "B1202      0       0      0        0        0       0      0      0      0   \n",
      "\n",
      "       Parody  ...  Vampire  Thriller  Shounen Ai  Action  Slice of Life  \\\n",
      "B1814       0  ...        0         0           0       0              0   \n",
      "B6470       0  ...        0         0           0       0              0   \n",
      "B893        0  ...        0         0           0       1              0   \n",
      "B1991       0  ...        0         0           0       0              0   \n",
      "B1150       0  ...        0         0           0       0              0   \n",
      "...       ...  ...      ...       ...         ...     ...            ...   \n",
      "B5021       0  ...        0         0           0       0              0   \n",
      "B6078       0  ...        0         0           0       0              0   \n",
      "B469        0  ...        0         0           0       1              0   \n",
      "B6254       0  ...        0         0           0       0              0   \n",
      "B1202       0  ...        0         0           0       0              0   \n",
      "\n",
      "       Harem  Shoujo Ai  Shounen  Psychological  Horror  \n",
      "B1814      0          0        0              0       0  \n",
      "B6470      0          0        0              0       0  \n",
      "B893       0          0        0              0       0  \n",
      "B1991      0          0        0              0       0  \n",
      "B1150      0          0        0              0       0  \n",
      "...      ...        ...      ...            ...     ...  \n",
      "B5021      0          0        1              0       0  \n",
      "B6078      0          0        0              0       0  \n",
      "B469       0          0        0              0       0  \n",
      "B6254      0          0        0              0       0  \n",
      "B1202      0          0        0              0       0  \n",
      "\n",
      "[7157 rows x 43 columns]\n",
      "(7157, 43)\n",
      "{'steps': [], 2: [array([5.58091634, 5.43135962, 5.28305657, 5.1363614 , 4.9916426 ,\n",
      "       4.84935231, 4.7098583 , 4.57310357, 4.43888284, 4.30729445,\n",
      "       4.17804758])], 3: [array([5.17799143, 5.06446201, 4.95247858, 4.84281832, 4.73554317,\n",
      "       4.63088844, 4.52961122, 4.43158833, 4.33672844, 4.24473948,\n",
      "       4.15533452])], 4: [array([5.48876178, 5.35909066, 5.2312731 , 5.10563318, 4.98265583,\n",
      "       4.86282208, 4.74642856, 4.6333333 , 4.52320528, 4.41621278,\n",
      "       4.31210283])], 5: [array([5.19549201, 5.0461374 , 4.89934304, 4.75894736, 4.6223886 ,\n",
      "       4.49157956, 4.36568361, 4.24743921, 4.13535561, 4.03622067,\n",
      "       3.94966129])], 6: [array([4.63250514, 4.52221879, 4.41628062, 4.31568033, 4.2224056 ,\n",
      "       4.13557732, 4.05480815, 3.98248716, 3.91535673, 3.85553481,\n",
      "       3.80059121])], 7: [array([4.30816339, 4.21032529, 4.1174795 , 4.02919645, 3.94859975,\n",
      "       3.87655057, 3.81302338, 3.77531922, 3.75141676, 3.73214788,\n",
      "       3.71661135])], 8: [array([4.88492586, 4.78911819, 4.69586561, 4.60561196, 4.51867398,\n",
      "       4.43505537, 4.35523958, 4.27878898, 4.20507091, 4.13437124,\n",
      "       4.06626406])], 9: [array([4.99190719, 4.86487398, 4.74027388, 4.63037615, 4.52908088,\n",
      "       4.45665598, 4.39001513, 4.32635763, 4.2653658 , 4.20725644,\n",
      "       4.15178319])], 10: [array([4.84876779, 4.71491425, 4.58335093, 4.45662588, 4.33504827,\n",
      "       4.21810475, 4.10799603, 4.00541385, 3.90984735, 3.84767358,\n",
      "       3.82420645])], 11: [array([4.60268528, 4.53694139, 4.47495183, 4.41600386, 4.36296738,\n",
      "       4.31536999, 4.27394513, 4.23627963, 4.20561839, 4.17854821,\n",
      "       4.15367196])], 12: [array([4.42302657, 4.31601659, 4.21521754, 4.12030776, 4.03212096,\n",
      "       3.9558523 , 3.89095983, 3.83839754, 3.79346664, 3.75696169,\n",
      "       3.72459995])], 13: [array([5.25006833, 5.12367618, 4.99994476, 4.87921531, 4.76200792,\n",
      "       4.64840633, 4.53868785, 4.4323739 , 4.32955798, 4.23024685,\n",
      "       4.13384383])], 14: [array([4.59377721, 4.41889786, 4.27097021, 4.15228354, 4.08393844,\n",
      "       4.03802051, 3.99757531, 3.96344823, 3.93556285, 3.91361135,\n",
      "       3.89558752])], 15: [array([4.93427562, 4.82460827, 4.71817525, 4.61490738, 4.51623692,\n",
      "       4.42695607, 4.34370317, 4.26669965, 4.19623061, 4.13139799,\n",
      "       4.07175181])], 16: [array([4.9315218 , 4.80434834, 4.6812961 , 4.56161999, 4.44585971,\n",
      "       4.33536867, 4.23050193, 4.12971978, 4.03338468, 3.94094518,\n",
      "       3.85090322])], 17: [array([4.49347853, 4.35648464, 4.22941113, 4.14783209, 4.10079813,\n",
      "       4.0570062 , 4.02005295, 3.98689733, 3.95718217, 3.93192833,\n",
      "       3.90846458])], 18: [array([4.81883787, 4.67888828, 4.54380287, 4.41428045, 4.29013778,\n",
      "       4.17230694, 4.06283417, 3.95937811, 3.85828525, 3.76253384,\n",
      "       3.69962046])], 19: [array([5.03023983, 4.88615284, 4.74478198, 4.60584478, 4.46985571,\n",
      "       4.33728439, 4.20986045, 4.08696062, 3.97083966, 3.8889259 ,\n",
      "       3.84180034])], 20: [array([5.1696618 , 5.04545247, 4.92487652, 4.80970544, 4.70243494,\n",
      "       4.60001421, 4.50269208, 4.41401333, 4.32990382, 4.25102445,\n",
      "       4.17629019])], 21: [array([4.72858287, 4.54709667, 4.38067771, 4.22102465, 4.11667682,\n",
      "       4.0361522 , 3.95857321, 3.88418109, 3.81458638, 3.7467003 ,\n",
      "       3.6813377 ])], 22: [array([4.49856468, 4.40137554, 4.30657906, 4.21614904, 4.1344034 ,\n",
      "       4.06229193, 3.99330578, 3.93093114, 3.87356986, 3.81943997,\n",
      "       3.76913082])], 23: [array([5.06060477, 4.93891559, 4.82069336, 4.70721323, 4.59878605,\n",
      "       4.49511016, 4.39849341, 4.3088374 , 4.22583208, 4.14855806,\n",
      "       4.07547255])], 24: [array([5.12157974, 5.00000577, 4.88157266, 4.76697525, 4.65704419,\n",
      "       4.55071175, 4.44875009, 4.35145552, 4.258746  , 4.17127872,\n",
      "       4.08604102])], 25: [array([5.09303432, 4.95508739, 4.8455065 , 4.74659094, 4.65106193,\n",
      "       4.55957062, 4.47224082, 4.38887792, 4.30895388, 4.2332513 ,\n",
      "       4.16120623])]}\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "#from networkx.algorithms import bipartite\n",
    "from networkx.algorithms import bipartite\n",
    "from networkx.algorithms.community.quality import modularity\n",
    "#from networkx.algorithms.community import greedy_modularity_communities\n",
    "import community\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xlsxwriter\n",
    "import math\n",
    "import time\n",
    "G = nx.Graph()\n",
    "# Read the file and add edges between users and items\n",
    "with open('animeupdate.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        u1, v1, r= line.strip().split('\\t')\n",
    "        u=str(\"A\")+u1\n",
    "        v=str(\"B\")+v1\n",
    "        G.add_node(u, bipartite=0)\n",
    "        G.add_node(v, bipartite=1)\n",
    "        G.add_edge(u, v)\n",
    "\n",
    "# Extract two node sets of the bipartite graph\n",
    "users = set(n for n, d in G.nodes(data=True) if d[\"bipartite\"] == 0)\n",
    "items = set(G) - users\n",
    "\n",
    "def matrix_factorization(R, P, Q, K, mat,steps=5, alpha=0.005, beta=0.02):   # function declaration of matrix factorization\n",
    "    '''\n",
    "    R: rating matrix\n",
    "    P: |U| * K (User features matrix)\n",
    "    Q: |D| * K (Item features matrix)\n",
    "    K: latent features\n",
    "    steps: iterations\n",
    "    alpha: learning rate\n",
    "    beta: regularization parameter'''\n",
    "    s=[]                       # list for storing steps\n",
    "    err=[]                     #  list for storing error\n",
    "    #rms=[]\n",
    "    Q = Q.T                    # calculating Q transpose\n",
    "    c=1                        # incrementing the count\n",
    "    #print(\"Q Tranpose\")\n",
    "    #print(Q)                   # printing Q Transpose\n",
    "    mae=np.zeros(steps)       # initialising all zeroes to rmse value\n",
    "    RSE=np.zeros(steps)        # initialising all zeroes to RSE value\n",
    "\n",
    "    for step in range(steps):\n",
    "        s.append(step)         # appending steps to the steps list s\n",
    "        for i in range(len(R)):\n",
    "            for j in range(len(R[i])):\n",
    "                if R[i][j] > 0:\n",
    "                    # calculate error\n",
    "                    eij = R[i][j] - np.dot(P[i,:],Q[:,j])   # calculating prediction error\n",
    "                    #print(eij)\n",
    "\n",
    "                    for k in range(K):\n",
    "                        # calculate gradient with a and beta parameter\n",
    "                        P[i][k] = P[i][k] + alpha * (2 * eij * Q[k][j] - beta * P[i][k])   # updating the value of p\n",
    "                        Q[k][j] = Q[k][j] + alpha * (2 * eij * P[i][k] - beta * Q[k][j])   # updating the value of q\n",
    "                        \n",
    "        #err.append(eij)\n",
    "        eR = np.dot(P,Q)      # calculating dot product of p & q\n",
    "        c=c+1                 # incrementing the count value\n",
    "        e = 0\n",
    "\n",
    "        for i in range(len(R)):\n",
    "\n",
    "            for j in range(len(R[i])):\n",
    "\n",
    "                if R[i][j] > 0:\n",
    "\n",
    "                    e = e + pow(R[i][j] - np.dot(P[i,:],Q[:,j]), 2)    # calculation of squared error\n",
    "\n",
    "                    for k in range(K):\n",
    "\n",
    "                        e = e + (beta) * (pow(P[i][k],2) + pow(Q[k][j],2))    # calculation of Regularised Square Error\n",
    "        \n",
    "        err.append(e)      # appending error to the error list err\n",
    "        #print(err)\n",
    "        PR = np.dot(P, Q)\n",
    "        #print(type(PR))\n",
    "        #PR=np.array(PR)\n",
    "        sum=0\n",
    "        for i in range(len(PR)):\n",
    "            for j in range(len(R[0])):\n",
    "                sum = sum+pow(PR[i][j] - R[i][j],2)\n",
    "        mae[step]=math.sqrt(sum/(len(R[0])*len(R)))    # calculation of RMSE\n",
    "        #print(\"step count\",c)\n",
    "        #rms.append(rmse)\n",
    "        #print(\"RMSE\",rmse)\n",
    "        # 0.001: local minimum\n",
    "        if e < 0.001:\n",
    "\n",
    "            break\n",
    "\n",
    "    #rmse.append(rmse)\n",
    "#     print(\"step count\",c)\n",
    "#     print(\"Regularised Square error\",e)\n",
    "#     print(\"RMSE\",rmse)\n",
    "#     for i in range(1,25):\n",
    "#         workbook = xlsxwriter.Workbook('RMSE'+str(i)+str(communities)+str(K)+str(mat)+'.xlsx')    # work  book for storing the result in excel sheet\n",
    "#         worksheet = workbook.add_worksheet()\n",
    "#         my_dict = {'Steps': s,                     # adding steps,error and rms into the dictionary\n",
    "#                'Error': err,\n",
    "#                'RMS': rmse}\n",
    "\n",
    "#         #print(\"my dict\",my_dict)\n",
    "#         col_num = 0\n",
    "#         for key, value in my_dict.items():\n",
    "#             worksheet.write(0, col_num, key)\n",
    "#             worksheet.write_column(1, col_num, value)  \n",
    "#             col_num += 1\n",
    "\n",
    "\n",
    "#        workbook.close()\n",
    "        return P, Q.T\n",
    "def louvain_algorithm(G, k):\n",
    "    # First, run the initial Louvain algorithm to obtain the first partition\n",
    "    partition = community.community_louvain.best_partition(G)\n",
    "    \n",
    "    # Initialize the list of partitions\n",
    "    partitions = [list() for _ in range(max(partition.values())+1)]\n",
    "    \n",
    "    # Assign each node to its corresponding community\n",
    "    for node, community_id in partition.items():\n",
    "        partitions[community_id].append(node)\n",
    "    \n",
    "    # Iterate until the desired number of components is reached\n",
    "    while len(partitions) < k:\n",
    "        # Compute the modularity score for the current partition\n",
    "        #prev_modularity = community.modularity(partition, G)\n",
    "        prev_modularity = community.modularity(partition, G)\n",
    "        # Iterate over each community in the current partition\n",
    "        for community_id in range(len(partitions)):\n",
    "            # Create a subgraph of the current community\n",
    "            subgraph = G.subgraph(partitions[community_id])\n",
    "            \n",
    "            # Compute the modularity score for each possible new community for this subgraph\n",
    "            best_modularity = -1\n",
    "            best_community_id = community_id\n",
    "            for neighbor in subgraph.nodes:\n",
    "                neighbor_community_id = partition[neighbor]\n",
    "                if neighbor_community_id != community_id:\n",
    "                    new_partition = partition.copy()\n",
    "                    new_partition[neighbor] = community_id\n",
    "                    #new_modularity = community.modularity(new_partition, G)\n",
    "                    new_modularity = community.modularity(new_partition, G)\n",
    "                    if new_modularity > best_modularity:\n",
    "                        best_modularity = new_modularity\n",
    "                        best_community_id = neighbor_community_id\n",
    "            \n",
    "            # If a better partition was found, update the partition\n",
    "            if best_modularity > prev_modularity:\n",
    "                for node in partitions[community_id]:\n",
    "                    partition[node] = best_community_id\n",
    "                partitions[best_community_id].extend(partitions[community_id])\n",
    "                partitions[community_id].clear()\n",
    "        \n",
    "        # Compute the modularity score for the current partition\n",
    "        #new_modularity = community.modularity(partition, G)\n",
    "        new_modularity = community.modularity(partition, G)\n",
    "        # If no further improvement can be made, break out of the loop\n",
    "        if new_modularity == prev_modularity:\n",
    "            break\n",
    "    \n",
    "    # Remove any empty partitions and return the final list of partitions\n",
    "    partitions = [p for p in partitions if p]\n",
    "    \n",
    "    # If the number of partitions is greater than k, merge partitions until only k remain\n",
    "    while len(partitions) > k:\n",
    "        # Compute the modularity score for each possible pair of partitions to merge\n",
    "        best_modularity = -1\n",
    "        best_partition_ids = None\n",
    "        for i in range(len(partitions)):\n",
    "            for j in range(i+1, len(partitions)):\n",
    "                new_partition = partition.copy()\n",
    "                for node in partitions[i]:\n",
    "                    new_partition[node] = j\n",
    "                #new_modularity = community.modularity(new_partition, G)\n",
    "                new_modularity = community.modularity(new_partition, G)\n",
    "                if new_modularity > best_modularity:\n",
    "                    best_modularity = new_modularity\n",
    "                    best_partition_ids = (i, j)\n",
    "        \n",
    "        # Merge the best pair of partitions\n",
    "        i, j = best_partition_ids\n",
    "        partition = {node: j if partition[node] == i else partition[node] for node in partition}\n",
    "        partitions[j].extend(partitions[i])\n",
    "        partitions[i].clear()\n",
    "        partitions = [p for p in partitions if p]\n",
    "    \n",
    "    # Return the final list of partitions\n",
    "    return partitions[:k]\n",
    "\n",
    "\n",
    "#print(G)\n",
    "df = pd.read_csv('animeupdate.txt', delimiter='\\t', header=None, names=['user', 'item', 'rating'])\n",
    "\n",
    "#data.rename(columns=lambda x: 'B'+ x, inplace = True)\n",
    "df['user'] = 'A' + df['user'].astype(str)\n",
    "df['item'] = 'B' + df['item'].astype(str)\n",
    "#print(df)\n",
    "matrix = df.pivot(index='user', columns='item', values='rating')\n",
    "#print(matrix)\n",
    "# # Fill missing values with zeros\n",
    "matrix = matrix.fillna(0)\n",
    "\n",
    "# Print the resulting matrix\n",
    "#print(type(matrix))\n",
    "# Convert DataFrame to numpy array\n",
    "matrix_R = matrix.to_numpy()\n",
    "\n",
    "\n",
    "# Read the file content\n",
    "file_path = 'anime_info.txt'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    animes_data = file.readlines()\n",
    "    \n",
    "\n",
    "# Define the list of genres\n",
    "genres_list = [\n",
    "    \"Josei\", \"Hentai\", \"Ecchi\", \"Romance\", \"Samurai\", \"Sci-Fi\", \"Magic\", \"Drama\", \"Mecha\", \"Parody\", \"Demons\", \n",
    "\"Game\", \"Seinen\", \"Martial Arts\", \"Yuri\", \"Dementia\", \"Shoujo\", \"Military\", \"Fantasy\", \"Adventure\", \"Historical\", \n",
    "\"Yaoi\", \"Music\", \"Sports\", \"Super Power\", \"Kids\", \"Space\", \"Police\", \"Mystery\", \"Cars\", \"Comedy\", \"Supernatural\", \n",
    "\"School\", \"Vampire\", \"Thriller\", \"Shounen Ai\", \"Action\", \"Slice of Life\", \"Harem\", \"Shoujo Ai\", \"Shounen\", \n",
    "\"Psychological\", \"Horror\"\n",
    "]\n",
    "\n",
    "# Convert the 'items' set to a list\n",
    "items_list=list(items)\n",
    "df1 = pd.DataFrame(0, index=items_list, columns=genres_list) \n",
    "#print(df1)\n",
    "\n",
    "\n",
    "for line in animes_data:\n",
    "    parts = line.strip().split(\"\\t\")\n",
    "    animes_id = str(parts[0])  # Convert to 0-based index\n",
    "    genres = parts[2].split(',')\n",
    "    #print(genres)\n",
    "    \n",
    "    for genre in genres:\n",
    "            if 'B'+animes_id in items and genre in genres_list:\n",
    "                df1.at['B'+animes_id, str(genre)] = 1\n",
    "\n",
    "\n",
    "#print(\"genre\",df1)\n",
    "genre_data=df1\n",
    "print('genre data', genre_data)\n",
    "print(genre_data.shape)\n",
    "\n",
    "\n",
    "\n",
    "max_communities=25\n",
    "iterations=25\n",
    "D={}\n",
    "D[\"steps\"]=[]\n",
    "#cramse={}\n",
    "#c=1\n",
    "for com in range(2, max_communities+1):\n",
    "    D[com]=[]\n",
    "    avg=0\n",
    "    rows, cols = (iterations,10)\n",
    "    arr = [[0]*cols]*rows\n",
    "    for it in range(0, iterations):           \n",
    "        P = louvain_algorithm(G, com)\n",
    "        #print(P)\n",
    "        for part in P:\n",
    "        #print(\"part matrix1\",part)\n",
    "            part_users = [u for u in part if u in matrix.index]\n",
    "            part_items = [i for i in part if i in matrix.columns]\n",
    "            #print(\"Users\",part_users)\n",
    "            #print(\"items\",part_items)\n",
    "            part_matrix = matrix.loc[part_users, part_items]\n",
    "            part_genre=genre_data.loc[part_items,genres_list]\n",
    "            R = part_matrix.to_numpy()\n",
    "            genre=part_genre.to_numpy()\n",
    "            #print(\"R\",R.shape)\n",
    "            #print(\"genre\", genre.shape)\n",
    "            # Construct the user profile matrix\n",
    "            user_profile_matrix = np.zeros((len(part_users), 43))\n",
    "            for user_index in range(len(part_users)):\n",
    "                for genre_index in range(43):\n",
    "                    user_profile_matrix[user_index, genre_index] = np.dot(R[user_index, :], genre[:, genre_index])\n",
    "            #print(\"user_profile_matrix\", user_profile_matrix.shape)\n",
    "            dotproduct_matrix = np.dot(user_profile_matrix, genre.T)\n",
    "            #print(\"dotproduct_matrix\", dotproduct_matrix.shape)\n",
    "            user_magnitudes = np.sqrt(np.sum(user_profile_matrix**2, axis=1))\n",
    "            #print(\"usermagniudes\", user_magnitudes.shape)\n",
    "            item_magnitudes = np.sqrt(np.sum(genre**2, axis=1))\n",
    "            #print(\"itemmagniudes\", item_magnitudes.shape)\n",
    "            numerator_matrix = dotproduct_matrix\n",
    "            cosine_similarity_matrix = np.zeros_like(dotproduct_matrix, dtype=float)\n",
    "            for user_index in range(len(part_users)):\n",
    "                for item_index in range(len(part_items)):\n",
    "                    denominator = user_magnitudes[user_index] * item_magnitudes[item_index]\n",
    "                    #print(denominator)\n",
    "                    if denominator!=0:\n",
    "                        cosine_similarity_matrix[user_index, item_index] = numerator_matrix[user_index, item_index] / denominator\n",
    "                    else:\n",
    "                        cosine_similarity_matrix[user_index, item_index] = 0\n",
    "                    # Display the cosine similarity matrix\n",
    "            #print(\"\\nCosine Similarity Matrix:\")\n",
    "            #print(cosine_similarity_matrix)\n",
    "            #print(cosine_similarity_matrix.shape)\n",
    "            # Scale the values from range 0-1 to range 1-5\n",
    "            min_val = 0\n",
    "            #print(min_val)\n",
    "            max_val = 1\n",
    "            #print(max_val)\n",
    "            ratingmin=1\n",
    "            ratingmax=10\n",
    "            updated_cosine_similarity_matrix = ratingmin + (ratingmax-ratingmin) * ((cosine_similarity_matrix - min_val) / (max_val - min_val))\n",
    "            #print(\"\\nupdated_Cosine Similarity Matrix:\")\n",
    "            #print(updated_cosine_similarity_matrix)\n",
    "            #print(updated_cosine_similarity_matrix.shape)        \n",
    "            N, M=R.shape\n",
    "                #print(N, M)\n",
    "            name=25\n",
    "            for K in range(10,11):\n",
    "                P = np.random.uniform(0.0,1.0,size=(N,K))\n",
    "                Q = np.random.uniform(0.0,1.0,size=(M,K))\n",
    "                nP, nQ= matrix_factorization(R, P, Q, K,name)\n",
    "            nR = np.dot(nP, nQ.T)\n",
    "        #print(\"Rating matrix for \"+str(name))\n",
    "        #print(nR)        \n",
    "        #print(nR.shape)\n",
    "        #cramse[c]=[]\n",
    "        #sum2=0\n",
    "        L=[]\n",
    "        for w1 in [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]:\n",
    "            sum2=0\n",
    "            pred_matrix_final=w1*(updated_cosine_similarity_matrix)+(1-w1)*(nR)\n",
    "            for i in range(len(pred_matrix_final)):\n",
    "                for j in range(len(pred_matrix_final[0])):\n",
    "                    sum2 = sum2+abs(pred_matrix_final[i][j] - R[i][j])\n",
    "            MAE_final=(sum2/(len(R[0])*len(R)))      \n",
    "            #print(\"iteration\",it,\"RMSE value\",RMSE_final)\n",
    "            L.append(MAE_final)\n",
    "        #c=c+1\n",
    "        #D[w1]=[]\n",
    "        arr[it]=L\n",
    "    D[com].append(np.average(arr, axis=0))\n",
    "    #print(cramse)\n",
    "print(D)\n",
    "#df = pd.DataFrame.from_dict({(i,j): D[i][j]\n",
    "#                             for i in D.keys()\n",
    "#                             for j in range(len(D[i]))},\n",
    "#                            orient='index')\n",
    "# Create a Pandas Excel writer using xlsxwriter as the engine\n",
    "writer = pd.ExcelWriter(\"25iterationscombinedLouMFcosine_anime_k=10_MAE.xlsx\", engine='xlsxwriter')\n",
    "\n",
    "# Loop over the dictionary keys\n",
    "for key in D.keys():\n",
    "    # Convert the dictionary subset to a DataFrame\n",
    "    df = pd.DataFrame(D[key])\n",
    "\n",
    "    # Write DataFrame to a sheet named 'Sheet_i'\n",
    "    df.to_excel(writer, sheet_name=f'Sheet_{key}', index=False)\n",
    "\n",
    "# Save the Excel file\n",
    "writer.close()\n",
    "#df =pd.DataFrame(D)\n",
    "#print(df)\n",
    "#df.to_csv('combinedLouMFcosine_ml-100K_k=10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f7631d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
